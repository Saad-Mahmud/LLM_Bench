{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7qTZbUcg5VD"
      },
      "source": [
        "<h3 align=\"center\"></h3>\n",
        "\n",
        "<h1 align=\"center\">Qwen 0.5b on GRPO</h1>\n",
        "\n",
        "---\n",
        "\n",
        "<h1 align=\"center\">Training a small math reasoner with RL</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV4W0sp1UWKe"
      },
      "source": [
        "This notebook is an alternate version of the [GRPO demo](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) by [will brown,](https://x.com/willccbb) training llama-1b on the gsm8k math dataset.\n",
        "\n",
        "We've only implemented a series of changes to make the code more workable on Colab:\n",
        "* Replacement of llama-1b with Qwen-0.5b\n",
        "* Generation with vllm, which yields a significant speed-up. Qwen small size makes it possible to run vllm on the same gpu as the one being used for GRPO.\n",
        "* Dropping flash-attn (recurrent bug with modeling qwen, not clear why)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPYBrSbY79we"
      },
      "source": [
        "## Setting up the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOMhew_59RbM"
      },
      "source": [
        "First we install vllm. Notice that you'll have to restart the session afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PYykgnUJ0BdB",
        "outputId": "a89e77ea-f4eb-499d-be6f-05972ac0e8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vllm in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (0.6.2)\n",
            "Collecting vllm\n",
            "  Downloading vllm-0.7.1-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: psutil in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (6.0.0)\n",
            "Requirement already satisfied: sentencepiece in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (4.66.4)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (9.0.0)\n",
            "Collecting transformers>=4.48.2 (from vllm)\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: protobuf in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (4.25.4)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.115.0)\n",
            "Requirement already satisfied: aiohttp in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (3.9.5)\n",
            "Collecting openai>=1.52.0 (from vllm)\n",
            "  Downloading openai-1.61.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn[standard] in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.30.6)\n",
            "Requirement already satisfied: pydantic>=2.9 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (2.9.2)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: pillow in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (10.4.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (7.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.7.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.10.9-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lark==1.2.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (1.2.2)\n",
            "Collecting xgrammar>=0.1.6 (from vllm)\n",
            "  Downloading xgrammar-0.1.11-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (4.12.2)\n",
            "Collecting filelock>=3.16.1 (from vllm)\n",
            "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: partial-json-parser in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.2.1.1.post4)\n",
            "Requirement already satisfied: pyzmq in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (26.2.0)\n",
            "Requirement already satisfied: msgspec in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.18.6)\n",
            "Requirement already satisfied: gguf==0.10.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib_metadata in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (8.5.0)\n",
            "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm)\n",
            "  Downloading mistral_common-1.5.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyyaml in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=74.1.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (75.1.0)\n",
            "Requirement already satisfied: einops in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (0.8.0)\n",
            "Collecting compressed-tensors==0.9.0 (from vllm)\n",
            "  Downloading compressed_tensors-0.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (3.0.0)\n",
            "Requirement already satisfied: ray>=2.9 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray[default]>=2.9->vllm) (2.37.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from vllm) (12.560.30)\n",
            "Collecting torch==2.5.1 (from vllm)\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchaudio==2.5.1 (from vllm)\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm)\n",
            "  Downloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: interegular in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.4)\n",
            "Requirement already satisfied: nest_asyncio in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
            "Requirement already satisfied: jsonschema in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Downloading airportsdata-20241001-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch==2.5.1->vllm) (3.3)\n",
            "Requirement already satisfied: fsspec in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch==2.5.1->vllm) (2024.3.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch==2.5.1->vllm) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->vllm)\n",
            "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch==2.5.1->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.38.6)\n",
            "Requirement already satisfied: packaging in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.1)\n",
            "Collecting opencv-python-headless<5.0.0,>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (4.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
            "Requirement already satisfied: click>=7.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.4.1)\n",
            "Collecting aiohttp-cors (from ray[default]>=2.9->vllm)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]>=2.9->vllm)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default]>=2.9->vllm)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default]>=2.9->vllm)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting smart-open (from ray[default]>=2.9->vllm)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]>=2.9->vllm)\n",
            "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from ray[default]>=2.9->vllm) (1.65.4)\n",
            "Collecting memray (from ray[default]>=2.9->vllm)\n",
            "  Downloading memray-1.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->vllm) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->vllm) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->vllm) (1.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from tiktoken>=0.6.0->vllm) (2024.5.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from tokenizers>=0.19.1->vllm) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=4.48.2->vllm) (0.4.3)\n",
            "Collecting pybind11 (from xgrammar>=0.1.6->vllm)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytest (from xgrammar>=0.1.6->vllm)\n",
            "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from uvicorn[standard]->vllm) (13.1)\n",
            "Requirement already satisfied: httpcore==1.* in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.52.0->vllm) (1.0.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (4.3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (3.0.1)\n",
            "Requirement already satisfied: rich>=11.2.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from memray->ray[default]>=2.9->vllm) (13.7.1)\n",
            "Collecting textual>=0.41.0 (from memray->ray[default]>=2.9->vllm)\n",
            "  Downloading textual-1.0.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]>=2.9->vllm)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from opencensus->ray[default]>=2.9->vllm) (2.19.1)\n",
            "Collecting iniconfig (from pytest->xgrammar>=0.1.6->vllm)\n",
            "  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pluggy<2,>=1.5 (from pytest->xgrammar>=0.1.6->vllm)\n",
            "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting wrapt (from smart-open->ray[default]>=2.9->vllm)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.63.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (2.33.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from rich>=11.2.0->memray->ray[default]>=2.9->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from rich>=11.2.0->memray->ray[default]>=2.9->vllm) (2.19.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]>=2.9->vllm) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]>=2.9->vllm)\n",
            "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting mdit-py-plugins (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]>=2.9->vllm)\n",
            "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]>=2.9->vllm)\n",
            "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.6.0)\n",
            "Downloading vllm-0.7.1-cp38-abi3-manylinux1_x86_64.whl (264.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.2/264.2 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.0-py3-none-any.whl (96 kB)\n",
            "Downloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
            "Downloading lm_format_enforcer-0.10.9-py3-none-any.whl (43 kB)\n",
            "Downloading mistral_common-1.5.2-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.61.0-py3-none-any.whl (460 kB)\n",
            "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.11-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
            "Downloading blake3-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (374 kB)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading airportsdata-20241001-py3-none-any.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "Downloading memray-1.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
            "Downloading textual-1.0.0-py3-none-any.whl (660 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.5/660.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
            "Downloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
            "Downloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, distlib, colorful, blake3, wrapt, uc-micro-py, pybind11, pluggy, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, iniconfig, filelock, astor, airportsdata, virtualenv, triton, smart-open, pytest, nvidia-cusolver-cu12, mdit-py-plugins, linkify-it-py, depyf, torch, openai, lm-format-enforcer, aiohttp-cors, xformers, transformers, torchvision, torchaudio, textual, outlines_core, opencensus, mistral_common, xgrammar, outlines, memray, compressed-tensors, vllm\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.15.4\n",
            "    Uninstalling filelock-3.15.4:\n",
            "      Successfully uninstalled filelock-3.15.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0\n",
            "    Uninstalling torch-2.4.0:\n",
            "      Successfully uninstalled torch-2.4.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.49.0\n",
            "    Uninstalling openai-1.49.0:\n",
            "      Successfully uninstalled openai-1.49.0\n",
            "  Attempting uninstall: lm-format-enforcer\n",
            "    Found existing installation: lm-format-enforcer 0.10.6\n",
            "    Uninstalling lm-format-enforcer-0.10.6:\n",
            "      Successfully uninstalled lm-format-enforcer-0.10.6\n",
            "  Attempting uninstall: xformers\n",
            "    Found existing installation: xformers 0.0.27.post2\n",
            "    Uninstalling xformers-0.0.27.post2:\n",
            "      Successfully uninstalled xformers-0.0.27.post2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.0\n",
            "    Uninstalling transformers-4.48.0:\n",
            "      Successfully uninstalled transformers-4.48.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0\n",
            "    Uninstalling torchvision-0.19.0:\n",
            "      Successfully uninstalled torchvision-0.19.0\n",
            "  Attempting uninstall: mistral_common\n",
            "    Found existing installation: mistral_common 1.4.3\n",
            "    Uninstalling mistral_common-1.4.3:\n",
            "      Successfully uninstalled mistral_common-1.4.3\n",
            "  Attempting uninstall: outlines\n",
            "    Found existing installation: outlines 0.0.46\n",
            "    Uninstalling outlines-0.0.46:\n",
            "      Successfully uninstalled outlines-0.0.46\n",
            "  Attempting uninstall: vllm\n",
            "    Found existing installation: vllm 0.6.2\n",
            "    Uninstalling vllm-0.6.2:\n",
            "      Successfully uninstalled vllm-0.6.2\n",
            "Successfully installed aiohttp-cors-0.7.0 airportsdata-20241001 astor-0.8.1 blake3-1.0.4 colorful-0.5.6 compressed-tensors-0.9.0 depyf-0.18.0 distlib-0.3.9 filelock-3.17.0 iniconfig-2.0.0 linkify-it-py-2.0.3 lm-format-enforcer-0.10.9 mdit-py-plugins-0.4.2 memray-1.15.0 mistral_common-1.5.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 openai-1.61.0 opencensus-0.11.4 opencensus-context-0.1.3 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 pluggy-1.5.0 py-spy-0.4.0 pybind11-2.13.6 pytest-8.3.4 smart-open-7.1.0 textual-1.0.0 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 transformers-4.48.2 triton-3.1.0 uc-micro-py-1.0.3 virtualenv-20.29.1 vllm-0.7.1 wrapt-1.17.2 xformers-0.0.28.post3 xgrammar-0.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install -U vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJxgfykz93lG"
      },
      "source": [
        "Then we install trl and datasets. It has to be in this order for some reason (bug on trl if you do vllm afterwards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybtxR89X1YJq",
        "outputId": "0cd9cc87-a2d0-46f7-b24d-f4d5ead6c888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trl in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (0.8.6)\n",
            "Collecting trl\n",
            "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: datasets in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (2.20.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from trl) (1.3.0)\n",
            "Requirement already satisfied: rich in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from trl) (13.7.1)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from trl) (4.48.0)\n",
            "Requirement already satisfied: filelock in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (6.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: sympy in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: networkx in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (75.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages (from sympy->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Downloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Installing collected packages: datasets, trl\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.20.0\n",
            "    Uninstalling datasets-2.20.0:\n",
            "      Successfully uninstalled datasets-2.20.0\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.8.6\n",
            "    Uninstalling trl-0.8.6:\n",
            "      Successfully uninstalled trl-0.8.6\n",
            "Successfully installed datasets-3.2.0 trl-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U trl datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJNTq5HG-EYI"
      },
      "source": [
        "## Defining the RL rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbej_WBE6wLV"
      },
      "source": [
        "Now we have everything ready to set up our RL training set and reward policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwqrjX1_-J3s"
      },
      "source": [
        "First we set the general prompt structure (with the reasoning tags)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q04kVVaQ6dSe",
        "outputId": "532f8722-ea3c-49c9-eec9-82a3af148248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-02 05:36:22 __init__.py:183] Automatically detected platform cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-02 05:36:22,357\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Load and prep dataset\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38AVgA19-PMk"
      },
      "source": [
        "Now we import the gsm8k dataset and restructure it to fit into a conversational prompt format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fno7X8Fh-N6k"
      },
      "outputs": [],
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi-7Hs0T-YwB"
      },
      "source": [
        "We move on now to the reward functions. The most important one is the \"correctness\" function which acts as a verifier (comparison of model completions vs. answer). The three others are formatting functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BLCIyOzI0Gol"
      },
      "outputs": [],
      "source": [
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuz-LQOQ-vSN"
      },
      "source": [
        "We now set the training arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gGFFu5u4-3uV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n"
          ]
        }
      ],
      "source": [
        "model_name = \"allenai/OLMoE-1B-7B-0924\"\n",
        "\n",
        "output_dir=\"outputs/OLMoE-1B-7B-0924-GRPO\"\n",
        "run_name=\"OLMoE-1B-7B-0924-GRPO-gsm8k\"\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=1,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=16,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=100,\n",
        "    max_grad_norm=0.1,\n",
        "    log_on_each_node=False,\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=.3,\n",
        "    vllm_device=\"cuda:0\",\n",
        "    report_to=\"none\" #I'm disabling Wandb.\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REuVM0ep-4dd"
      },
      "source": [
        "And launch the actual training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "237d44c6ba514d628d8b106c31113f51",
            "fcf49a8b18c34c83af7a4b2231d34ac9",
            "a6e9dc143bcc46a3a142f47312021c2b",
            "fc95f56a843c47e1a189b438ffd2f54d",
            "d878defc0eeb4afa84d47389262b2dd4",
            "68396d95b92f4e42b2689522a11ac3bf",
            "f910e3eb2c7e40a69b439facbcf6e3a2",
            "970af5d3c35342c0b8792230254f7b08",
            "2576dab321ea428bb4f9da69272eb97c",
            "bc184dbe11ae436db0fbdee8659626c3",
            "e41fb61a3c0c42ea8251a4711833be5e"
          ]
        },
        "id": "U1ixGbPG0Ni-",
        "outputId": "ae4d7b91-33dd-40ed-ce62-ac5c1337373c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:301: UserWarning: The requested device cuda:0 is also used for training. This may lead to unexpected behavior. It is recommended to use a dedicated device for vLLM.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-02 05:36:40 config.py:526] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 02-02 05:36:40 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='allenai/OLMoE-1B-7B-0924', speculative_config=None, tokenizer='allenai/OLMoE-1B-7B-0924', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMoE-1B-7B-0924, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 02-02 05:36:40 cuda.py:235] Using Flash Attention backend.\n",
            "INFO 02-02 05:36:41 model_runner.py:1111] Starting to load model allenai/OLMoE-1B-7B-0924...\n",
            "INFO 02-02 05:36:41 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.00s/it]\n",
            "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.04s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.07it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.04it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-02 05:36:44 model_runner.py:1116] Loading model weights took 12.8890 GB\n",
            "WARNING 02-02 05:36:45 fused_moe.py:647] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/smahmud/anaconda3/envs/nlp/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1024,device_name=NVIDIA_RTX_6000_Ada_Generation.json\n",
            "INFO 02-02 05:36:45 worker.py:266] Memory profiling takes 0.94 seconds\n",
            "INFO 02-02 05:36:45 worker.py:266] the current vLLM instance can use total_gpu_memory (47.38GiB) x gpu_memory_utilization (0.30) = 14.21GiB\n",
            "INFO 02-02 05:36:45 worker.py:266] model weights take 12.89GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 0.76GiB.\n",
            "INFO 02-02 05:36:46 executor_base.py:108] # CUDA blocks: 390, # CPU blocks: 2048\n",
            "INFO 02-02 05:36:46 executor_base.py:113] Maximum concurrency for 4096 tokens per request: 1.52x\n",
            "INFO 02-02 05:36:47 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:20<00:00,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 02-02 05:37:08 model_runner.py:1563] Graph capturing finished in 21 secs, took 0.23 GiB\n",
            "INFO 02-02 05:37:08 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 23.90 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use peft at your own risk; not working for me with multi-GPU training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#peft_config=peft_config\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:376\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    374\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    375\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 376\u001b[0m prompts_text \u001b[38;5;241m=\u001b[39m [\u001b[43mmaybe_apply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    377\u001b[0m prompt_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class(\n\u001b[1;32m    378\u001b[0m     prompts_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    379\u001b[0m )\n\u001b[1;32m    380\u001b[0m prompt_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_prepare_inputs(prompt_inputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/trl/data_utils.py:201\u001b[0m, in \u001b[0;36mmaybe_apply_chat_template\u001b[0;34m(example, tokenizer, tools)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mIf the example is in a conversational format, apply a chat template to it.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_conversational(example):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/trl/data_utils.py:93\u001b[0m, in \u001b[0;36mapply_chat_template\u001b[0;34m(example, tokenizer, tools)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Apply the chat template to the prompt, adding the generation prompt\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m example:\n\u001b[0;32m---> 93\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Apply the chat template to the entire prompt + completion\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m example:  \u001b[38;5;66;03m# explicit prompt and prompt-completion case\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1621\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1621\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1624\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1789\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
          ]
        }
      ],
      "source": [
        "# use peft at your own risk; not working for me with multi-GPU training\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    #peft_config=peft_config\n",
        ")\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "237d44c6ba514d628d8b106c31113f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcf49a8b18c34c83af7a4b2231d34ac9",
              "IPY_MODEL_a6e9dc143bcc46a3a142f47312021c2b",
              "IPY_MODEL_fc95f56a843c47e1a189b438ffd2f54d"
            ],
            "layout": "IPY_MODEL_d878defc0eeb4afa84d47389262b2dd4"
          }
        },
        "2576dab321ea428bb4f9da69272eb97c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68396d95b92f4e42b2689522a11ac3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970af5d3c35342c0b8792230254f7b08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e9dc143bcc46a3a142f47312021c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_970af5d3c35342c0b8792230254f7b08",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2576dab321ea428bb4f9da69272eb97c",
            "value": 1
          }
        },
        "bc184dbe11ae436db0fbdee8659626c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d878defc0eeb4afa84d47389262b2dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41fb61a3c0c42ea8251a4711833be5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f910e3eb2c7e40a69b439facbcf6e3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc95f56a843c47e1a189b438ffd2f54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc184dbe11ae436db0fbdee8659626c3",
            "placeholder": "​",
            "style": "IPY_MODEL_e41fb61a3c0c42ea8251a4711833be5e",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  3.17it/s]\n"
          }
        },
        "fcf49a8b18c34c83af7a4b2231d34ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68396d95b92f4e42b2689522a11ac3bf",
            "placeholder": "​",
            "style": "IPY_MODEL_f910e3eb2c7e40a69b439facbcf6e3a2",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
